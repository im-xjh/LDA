{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ed54ea",
   "metadata": {},
   "source": [
    "### JSONL 文本分词处理\n",
    "该脚本用于对 JSONL 文件中的文本进行分词处理，并输出分词结果到新的 JSONL 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f10828",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eeb583",
   "metadata": {},
   "source": [
    "### 加载停用词表\n",
    "该函数从指定的停用词文件加载停用词列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe6eb1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_stopwords(stopword_path):\n",
    "    \"\"\"\n",
    "    从停用词文件加载停用词列表\n",
    "    \"\"\"\n",
    "    stopwords = set()\n",
    "    with open(stopword_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            w = line.strip()\n",
    "            if w:\n",
    "                stopwords.add(w)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3892d10",
   "metadata": {},
   "source": [
    "### 定义文本处理函数\n",
    "这里定义两个函数用于判断 token 是否为全中文或全英文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505e9c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_chinese(token):\n",
    "    \"\"\"判断 token 是否只包含中文字符\"\"\"\n",
    "    return re.fullmatch(r'[\\u4e00-\\u9fa5]+', token) is not None\n",
    "\n",
    "def is_english(token):\n",
    "    \"\"\"判断 token 是否只包含英文字母\"\"\"\n",
    "    return re.fullmatch(r'[A-Za-z]+', token) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd862849",
   "metadata": {},
   "source": [
    "### 配置参数\n",
    "配置输入 JSONL 文件、输出 JSONL 文件、自定义词典和停用词表路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_jsonl = \"\"\n",
    "output_jsonl = \"\"\n",
    "dict_file = \"\"\n",
    "stopword_file = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfaee7",
   "metadata": {},
   "source": [
    "### 加载自定义词典\n",
    "如果存在自定义词典，则加载，否则使用默认分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d59312",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dict_file):\n",
    "    jieba.load_userdict(dict_file)\n",
    "    print(f\"已加载自定义词典: {dict_file}\")\n",
    "else:\n",
    "    print(f\"未找到自定义词典: {dict_file}, 将使用默认分词\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72318836",
   "metadata": {},
   "source": [
    "### 加载停用词\n",
    "读取停用词文件，并加载到 `stopwords` 集合中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = load_stopwords(stopword_file)\n",
    "print(f\"已加载停用词，共 {len(stopwords)} 个\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f4541",
   "metadata": {},
   "source": [
    "### 读取 JSONL 并处理文本\n",
    "读取 JSONL 文件，进行分词，并存储结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "with open(input_jsonl, 'r', encoding='utf-8') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        record = json.loads(line)\n",
    "        text = record.get(\"content\", \"\")\n",
    "\n",
    "        # 使用 jieba 分词\n",
    "        tokens = jieba.lcut(text)\n",
    "        tokens_filtered = []\n",
    "        for token in tokens:\n",
    "            # 去除零宽字符\n",
    "            token = re.sub(r'[\\u200b\\u200c\\u200d\\ufeff]', '', token)\n",
    "            token = token.strip()\n",
    "            if not token or token in stopwords:\n",
    "                continue\n",
    "            # 只保留全中文或全英文的 token\n",
    "            if is_chinese(token):\n",
    "                if 2 <= len(token) <= 6:\n",
    "                    tokens_filtered.append(token)\n",
    "            elif is_english(token):\n",
    "                if len(token) >= 3:\n",
    "                    tokens_filtered.append(token)\n",
    "            # 其他情况（数字、标点、表情或混合字符）均删除\n",
    "        \n",
    "        record[\"tokens\"] = tokens_filtered\n",
    "        data_list.append(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a9945",
   "metadata": {},
   "source": [
    "### 写入新的 JSONL 文件\n",
    "将处理后的数据写入新的 JSONL 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_jsonl, 'w', encoding='utf-8') as fout:\n",
    "    for rec in data_list:\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"分词完成，共处理 {len(data_list)} 条记录，结果已写入 {output_jsonl}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
